<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tensorflows on Mrlkun</title>
    <link>https://KUNeasy.github.io/tensorflow/</link>
    <description>Recent content in Tensorflows on Mrlkun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 12 Aug 2019 00:34:43 +0800</lastBuildDate>
    
	<atom:link href="https://KUNeasy.github.io/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Buildtensorflow</title>
      <link>https://KUNeasy.github.io/tensorflow/buildtensorflow/</link>
      <pubDate>Mon, 12 Aug 2019 00:34:43 +0800</pubDate>
      
      <guid>https://KUNeasy.github.io/tensorflow/buildtensorflow/</guid>
      <description>模块化搭建神经网络 神经网络搭建包括前向传播过程、反向传播过程、反向传播过程中用到的正则化、指数衰减学习率、滑动平均方法的设置、以及测试模块。
模块一 前向传播过程（forward.py） 前向传播过程中，需要定义神经网络中的参数 w 和偏置 b，定义由输入到输出的网络结构。通过定义函数 get_weight()实现对参数 w 的设置，包括参数 w 的形状和是否正则化的标志。同样，通过定义函数 get_bias()实现对偏置 b 的设置。
import tensorflow as tf INPUT_NODE = 784 OUTPUT_NODE = 10 LAYER1_NODE = 500 def get_weight(shape, regularizer): w = tf.Variable(tf.truncated_normal(shape,stddev=0.1)) if regularizer != None: tf.add_to_collection(&#39;losses&#39;, tf.contrib.layers.l2_regularizer(regularizer)(w)) return w def get_bias(shape): b = tf.Variable(tf.zeros(shape)) return b def forward(x, regularizer): w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer) b1 = get_bias([LAYER1_NODE]) y1 = tf.nn.relu(tf.matmul(x, w1) + b1) w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer) b2 = get_bias([OUTPUT_NODE]) y = tf.</description>
    </item>
    
  </channel>
</rss>